---
title: "Class Reference"
author: "Student name"
output:
  html_document:
    theme: cerulean
    highlight: pygments
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Four (Typical) Data Types
Numeric = a number
Character = "text" like county or business name, but can also be numbers like ZIP codes of phone numbers; identifiers
Date = fully-formed dates, like 2019-10-01; incomplete dates, like 2019, are numberic
Logical = yes/no, true/false, etc; rare(ish)

## General

!Set Working Directory! - do this first
setwd("~target/path/name")

Use /# to make comments like <-- --> in HTML or /* */ in CSS

**FILTER > GROUP_BY > SUMMARIZE > ARRANGE** - general flow for R

Making a code block always looks like:
```{r}

```

ppp_maryland_loans <- read_rds("ppp_maryland.rds")
[        1       ][ 2 ][   3  ][        4        ]
*variable* [1] name used to refer to some more complex thing
*variable assignment operator* [2] assigns a word to something
*function* [3] computer code that takes in information; follows series of pre-determined steps; spits it back out
*argument* [4] things put inside of function to customize what the function does

*x ~ y* = if it's x, "then" change it to y
*^abc* = if it starts with "abc"
*%>%* = "and then do this"
*data %>% function* = typical pattern, "take data and do this specific action"

data %>% group_by(COLUMN NAME) %>% summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))

## Functions & Commands

*glimpse(x)* = list of the columns, the data type for each column, first few values for each column
*head(x)* = print out columns & first six rows of data
*head(x)* = first 6 rows of dataset x
*colnames (x)* = names of all columns in dataset x
*summary(x)* = for numerics: minimum, 1st quartile, median, mean, 3rd quartile, and max of all columns in dataset x; for characters: number of rows
*group_by(x)* = group data together by column "x" character
*select(x,y,z)* = show all rows but only with columns x, y and z
*summarise(x)* = sets up any desired summaries
  EXAMPLE:
  ppp_maryland_loans %>%               - take this data set;
  group_by(project_county_name) %>%    - group rows together according to the names of the counties;
  summarise(                           - and summarize it like this:
    count_loans = n()                  - the number of rows (each individual loan) in each county name.
  )
*get_dupes()* = check for duplicates in first column in dataset
*distinct()* = find all different valued entries in first column in dataset

*n()* = the number of the summarized function
*nrow(x)* = number of rows in dataset "x"
*arrange(x)* = arrange according to a certain property; can have multiple targets to sort by one than the other
  EXAMPLES:
  arrange(desc(count_loans))           - highest number number of loans first
  arrange(project_county_name,race)    - loans in each county to each racial demographic
alphabetically
  arrange(percent_payroll, desc(amount)) - lowest payroll percentage, and within that, highest loan amount - *first column in arrange takes precedent, then second, and so on*
    arrange(desc())                    - highest to lowest
    arrange()                          - lowest to highest
    -default-                          - alphabetically

*sum(x)* = sum of numeric values in column "x"
*mean(x)* = average of numeric values in column "x"
*median(x)* = value that sits at the midpoint of all values in column "x"
*min(x)/max(x)* = lowest/highest value in column "x"

*mutate(x = y)* = create new column "x" with a value (typically a function) of "y"
  mutate(percent_payroll = (payroll_proceed/amount)*100) - find out what percent of each loan went towards payroll by dividing the raw amount sent to payroll by the total loan, and multiply by 100
  mutate(in_out = *if_else*(
    servicing_lender_state == 'MD', "IN", "OUT"
  ) = make new column "in_out" saying if s_l_s is or isn't in Maryland, remember double *==*
  maryland_jobs_categories <- maryland_ppp %>%
mutate(
  jobs_retained_category = *case_when*(
    jobs_retained < 10 ~ 'under_10',
    jobs_retained >= 10 & jobs_retained < 25 ~ '10_to_25',
    jobs_retained >= 50 & jobs_retained < 100 ~ '50_to_100',
    jobs_retained >= 100 & jobs_retained < 250 ~ '100_to_250',
    jobs_retained >= 250 & jobs_retained < 500 ~ '250_to_500',
    jobs_retained == 500 ~ '500'
  ) = new column based on ranges of values; similar to if_else but with >2 potential buckets
)
    (x = y ~ z) = If column x value is y, then change it to z
    str_detect(x, "^abc") ~ y = if value in column x starts with "abc" then change it to y
    TRUE ~ x = all other values in column x stay the same
  *across(x, as.y)* = change character type of column x to type y
    
  *CleanDate = ymd_hms(x)* - /lubridate only/ = parse dates more accurately without extra code - *read_csv* (instead of read_csv) also works; use *CleanDate = ymd(x)* if there's no time; MAKE SURE FILE EXTENSION MATCHES
  *floor_date(x, "y")* = rounds column "x" down to nearest boundary of specified time unit "y"
  *ceiling_date(x, "y")* = rounds column "x" up to nearest boundary of specified time unit "y"
  *str_to_upper/lower(x)* = change letters in of characters in column "x" to make all letters upper/lowercase
  *str_to_title(x)* = capitalize the first letter of every word
  *x=as.character(y)* = change data type of column x to a character with a value of y (can change value with formulas in "y" space); can also change to .numeric, .date, etc...
  *x = str_sub(x, start=yL, end=zL)* = change length of values in column X to start at Yth letter character from the left and end at Zth from the left; change to yR and zR to count from the right
  
*read_csv("y", guess_max=x* = use only x number of columns to try to guess the data type for each column in dataset y

*filter(x [y] z)*:  x = any column name from your dataframe; [y] = some comparison operator (==, >,  <); Z = something to compare the x to
  use /&/ to do more than one filter at once, e.g. <filter(project_county_name == "PRINCE GEORGES" & business_type == "Non-Profit Organization" & amount > 150000)>
  use /|/ to allow for more than one filter criteria, wherein a row that fits any one of the listed filters will be shown, e.g. <filter(project_county_name == "PRINCE GEORGES" | business_type == "Non-Profit Organization" | amount > 150000)>
  /Look at your data before you filter - names need to be EXACT, including capitalization, to be captured by filters/
  *is.na(x)* = filter all rows wherein column "x" value is NA; /this is the only way to target NA values, its not a number/
  
*clean_names()* = fixes the column headers in a dataset with janitor library
*rename(x = y)* = changes column header y to new header x

*bind_rows(list(x,y,z))* = combine datasets x, y and z to compare trends over time - must have exact same formatting
*x %>% left_join(y, by="z")* = join dataset x with datset y according to value in column z, while eliminating non-matching rows
  EXAMPLE
  maryland_ppp_with_naics %>% left_join(maryland_zcta, by=c("zip"="ZCTA5N"))
    merge ppp_with_naics with zcta, using different named columns zip and ZCTA5N
*x %>% right_join(y, by="z")* = join dataset x with dataset y according to values in column z, while still displaying rows that didnt have matching values

*c(x,y,z)* = combine items x, y, and z into a list test

## Web Scraping!

x %>% *read_html()* = read html from url "x"
x %>% read_html() %>% *html_table()* = extract and convert all html tables from url "x"
  - if multiple tables are extracted from a webpage, they will all be saved in the element created a la [[1]] [[2]] [[3]] etc
  - you can save a specific table from a list by doing y <- x[[2]], which will make "y" the 2nd table scrubbed from webpage "x"
y %>% read_html() %>% *html_element(xpath = x)* = grab the html element "x" from url "y" and get rid of everything else
*slice(x)* = remove all rows _except_ x
*slice(-x)* = remove row x
*slice(c(x,y))* = remove all rows _except_ x & y
*slice(-c(x,y))* = remove rows x & y

## Loop Hero

for (x in y) {
  function(x)                 
}

- The information inside the parentheses tells R what list to use -- "y" -- and how to identify list elements later on -- "x". Note - "y" has to be the exact name of the list you want to use. Make sure "x" is called the same thing in both places, as well.
- R will then perform function on each "x" in list "y".

for (number in 1:10) {
  function(number)
}

- This does the same thing as the above, but is specifically for iterating numbers.
- This is very useful for large tables, in the following manner: 1:nrow will repeat the function for each row in your table from 1 until the end.

mutate(x = *paste0*("https://www.bls.gov/iag/tgs/iag",y,".htm"))
  - This is a weird one but super useful. So, we're creating a new column called "x," and the value is going to be a url. The only part of the URL that changes in each row will be inside ",,", and that will be equal to the value in column "y" for each row. Make sense? Yes. SHUT UP YES IT DOES.
  
x %>% ... %>% *bind_cols(y)* = after you're done modifying dataset x, apend the appropriate columns from dataset y to the end of each column

## Using APIs - US Census

*load_variables(x, "y", cache = TRUE)* = load census dataset y from year x
  Datasets:
    sf1, sf3 - decennial census
    acs1, acs3, acs5 - annual census surveys
*get_decennial(geography = "x", variables = "y", year = z)* = load datatframe, where x is geographic area [state, county, place, tract, block], y is the specific dataframe from the sf/acs you loaded, and z is the year
*get_acs()* = same as above, but for annual surveys
  ALL ARGUMENTS
    geography	= The geography of your data.
    variables = Character string or vector of character strings of variable IDs.
    table = The Census table for which you would like to request all variables. Uses lookup tables to identify the variables; performs faster when variable table already exists through load_variables(cache = TRUE). Only one table may be requested per call.
    cache_table = Whether or not to cache table names for faster future access. Defaults to FALSE; if TRUE, only needs to be called once per dataset. If variables dataset is already cached via the load_variables function, this can be bypassed.
    year = The year for which you are requesting data. Defaults to 2010; 2000, 2010, and 2020 are available.
    sumfile = The Census summary file. Defaults to sf1; the function will look in sf3 if it cannot find a variable in sf1.
    state = The state for which you are requesting data. State names, postal codes, and FIPS codes are accepted. Defaults to NULL.
    county = The county for which you are requesting data. County names and FIPS codes are accepted. Must be combined with a value supplied to `state`. Defaults to NULL.
    geometry = if FALSE (the default), return a regular tibble of ACS data. if TRUE, uses the tigris package to return an sf tibble with simple feature geometry in the `geometry` column. state, county, tract, and block group are supported for 2000 through 2020; block and ZCTA geometry are supported for 2000 and 2010.
    output = One of "tidy" (the default) in which each row represents an enumeration unit-variable combination, or "wide" in which each row represents an enumeration unit and the variables are in the columns.
    keep_geo_vars = if TRUE, keeps all the variables from the Census shapefile obtained by tigris. Defaults to FALSE.
    shift_geo = (deprecated) if TRUE, returns geometry with Alaska and Hawaii shifted for thematic mapping of the entire US. Geometry was originally obtained from the albersusa R package. As of May 2021, we recommend using tigris::shift_geometry() instead.
    summary_var	= Character string of a "summary variable" from the decennial Census to be included in your output. Usually a variable (e.g. total population) that you'll want to use as a denominator or comparison.
    key = Your Census API key. Obtain one at https://api.census.gov/data/key_signup.html
    show_call	= if TRUE, display call made to Census API. This can be very useful in debugging and determining if error messages returned are due to tidycensus or the Census API. Copy to the API call into a browser and see what is returned by the API directly. Defaults to FALSE.

## Data Visualization

*ggplot()* = make a visualization - but we need more input
    $Inside$ - use "+" instead of "%>%"
*geom_bar* = make a bar chart
*geom_line* = make a line graph
*coord_flip* = flip axes in the chart
*theme_minimal* = minimalist theme :O
*scale_x_date* = scale dates on x axis
  *date_breaks= "x"* = set date breaks to every x unit of time
  *date_labels= "x"* = format date labels as x
    $Capital Letter$ = full length, e.g. %Y -> 2022, %B -> January
    $Lowercase Letter$ = abbreviated, e.g. %y -> '22, %b -> Jan

A E S T H E T I C
*aes(x)* = apply the following aesthetic changes to the visualization
  *reorder(x,y,z)* = reorder variables in the sequence of x, y, z
  *weight=x* = set height of bar to variable x
  
(https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/) > all the different themes you can add

## Mapping



## Other tricks

*str_c(x, y)* = concatenates elements into a single string "xy"
  ex: str_c(state_code, county_code) -> 24001

## Different Libraries

*lubridate* - for working with dates; "easier to do the things R does with date-times and possible to do the things R does not"
*tidyverse* - group of libraries that makes life easier
*ggthemes* - themes for data visualizations
*janitor* - makes your data easier to work with my fixing abnormalities in headers and data types
*rvest* - web scraping! Allah preserve us
*tidycensus* - API tool for the US Census
*tigris* - has a FIPS code table in it